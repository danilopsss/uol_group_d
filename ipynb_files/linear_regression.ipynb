{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Linear Regression\n",
    "\n",
    "With the Linear regression method, we are trying to predict the values for 2025\n",
    "\n",
    "For this we are going to first load the data from the dataset"
   ],
   "id": "45452c5c2ec368c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T00:24:07.315683Z",
     "start_time": "2024-10-01T00:24:05.762909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# File paths based on  datasets_mean_median.zip\n",
    "file_mean = \"../datasets/LAEI_2019_NA_FILLED_WITH_MEAN.csv\"\n",
    "file_median = \"../datasets/LAEI_2019_NA_FILLED_WITH_MEDIAN.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "mean_df = pd.read_csv(file_mean)\n",
    "median_df = pd.read_csv(file_median)\n",
    "\n",
    "# Printing the rows\n",
    "print(\"Data with Mean Imputation:\\n\", mean_df.head())\n",
    "print(\"\\nData with Median Imputation:\\n\", median_df.head())"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with Mean Imputation:\n",
      "    Year  Grid ID 2019  LAEI 1km2 ID  Easting  Northing  Borough     Zone  \\\n",
      "0  2030             1          5910   510500    203500  Non GLA  Non GLA   \n",
      "1  2030             2          5911   511500    203500  Non GLA  Non GLA   \n",
      "2  2030             3          5912   512500    203500  Non GLA  Non GLA   \n",
      "3  2030             4          5915   515500    203500  Non GLA  Non GLA   \n",
      "4  2030             5          5916   516500    203500  Non GLA  Non GLA   \n",
      "\n",
      "  Main Source Category   Sector        Source  ...  n2o  nh3  nmvoc  nox  pb  \\\n",
      "0             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "1             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "2             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "3             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "4             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "\n",
      "   pcb      pm10     pm2.5  so2  Emissions Unit  \n",
      "0  NaN  0.019183  0.019183  NaN    tonnes/annum  \n",
      "1  NaN  0.015719  0.015719  NaN    tonnes/annum  \n",
      "2  NaN  0.019878  0.019878  NaN    tonnes/annum  \n",
      "3  NaN  0.020946  0.020946  NaN    tonnes/annum  \n",
      "4  NaN  0.020105  0.020105  NaN    tonnes/annum  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Data with Median Imputation:\n",
      "    Year  Grid ID 2019  LAEI 1km2 ID  Easting  Northing  Borough     Zone  \\\n",
      "0  2030             1          5910   510500    203500  Non GLA  Non GLA   \n",
      "1  2030             2          5911   511500    203500  Non GLA  Non GLA   \n",
      "2  2030             3          5912   512500    203500  Non GLA  Non GLA   \n",
      "3  2030             4          5915   515500    203500  Non GLA  Non GLA   \n",
      "4  2030             5          5916   516500    203500  Non GLA  Non GLA   \n",
      "\n",
      "  Main Source Category   Sector        Source  ...  n2o  nh3  nmvoc  nox  pb  \\\n",
      "0             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "1             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "2             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "3             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "4             Domestic  Biomass  Wood Burning  ...  NaN  NaN    NaN  NaN NaN   \n",
      "\n",
      "   pcb      pm10     pm2.5  so2  Emissions Unit  \n",
      "0  NaN  0.019183  0.019183  NaN    tonnes/annum  \n",
      "1  NaN  0.015719  0.015719  NaN    tonnes/annum  \n",
      "2  NaN  0.019878  0.019878  NaN    tonnes/annum  \n",
      "3  NaN  0.020946  0.020946  NaN    tonnes/annum  \n",
      "4  NaN  0.020105  0.020105  NaN    tonnes/annum  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T00:24:07.871448Z",
     "start_time": "2024-10-01T00:24:07.320439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter dataset to include only years 2013, 2016, and 2019 for training\n",
    "train_data_mean = mean_df[mean_df['Year'].isin([2013, 2016, 2019])]\n",
    "train_data_median = median_df[median_df['Year'].isin([2013, 2016, 2019])]\n",
    "\n",
    "### Process Categorical Variables\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the Label Encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Encode 'Main Source Category' using explicit row and column indexing\n",
    "mean_source_encoded = encoder.fit_transform(train_data_mean.loc[:, \"Main Source Category\"])\n",
    "median_source_encoded = encoder.fit_transform(train_data_median.loc[:, \"Main Source Category\"])\n",
    "train_data_mean.loc[:, \"main_source_encoded\"] = mean_source_encoded\n",
    "train_data_median.loc[:, \"main_source_encoded\"] = median_source_encoded\n",
    "\n",
    "pollutants = [\"nox\", \"pm10\", \"pm2.5\", \"co2\"]\n",
    "\n",
    "\n",
    "# Fill missing values with the mean for the mean-imputed dataset\n",
    "for col in pollutants:\n",
    "    train_data_mean.loc[:, col] = train_data_mean[col].fillna(train_data_mean[col].mean())\n",
    "\n",
    "# Fill missing values with the median for the median-imputed dataset\n",
    "for col in pollutants:\n",
    "    train_data_median.loc[:, col] = train_data_median[col].fillna(train_data_median[col].median())\n",
    "\n",
    "### Prepare Data for Model Training\n",
    "# Define the years for training and testing\n",
    "train_years = [2013, 2016, 2019]\n",
    "test_year = 2025\n",
    "\n",
    "# Prepare the features (X) by dropping target variables and Year\n",
    "X_train_mean = train_data_mean[train_data_mean[\"Year\"].isin(train_years)].drop(columns=pollutants + [\"Year\"])\n",
    "X_test_mean = train_data_mean[train_data_mean[\"Year\"] == test_year].drop(columns=pollutants + [\"Year\"])\n",
    "\n",
    "X_train_median = train_data_median[train_data_median[\"Year\"].isin(train_years)].drop(columns=pollutants + [\"Year\"])\n",
    "X_test_median = train_data_median[train_data_median[\"Year\"] == test_year].drop(columns=pollutants + [\"Year\"])\n",
    "\n",
    "# Prepare the target variables (y) for both the training and testing sets\n",
    "# Using a dictionary to store the targets for each pollutant\n",
    "y_train_mean = {pollutant: train_data_mean[train_data_mean[\"Year\"].isin(train_years)][pollutant] for pollutant in pollutants}\n",
    "y_test_mean = {pollutant: train_data_mean[train_data_mean[\"Year\"] == test_year][pollutant] for pollutant in pollutants}\n",
    "\n",
    "y_train_median = {pollutant: train_data_median[train_data_median[\"Year\"].isin(train_years)][pollutant] for pollutant in pollutants}\n",
    "y_test_median = {pollutant: train_data_median[train_data_median[\"Year\"] == test_year][pollutant] for pollutant in pollutants}"
   ],
   "id": "8d0cdd2914ab9920",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Defining the function to train the model",
   "id": "2126eecb565f943d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T00:24:10.302631Z",
     "start_time": "2024-10-01T00:24:07.872880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def train_linear_regression_for_all_polutants(X_train_mean, X_train_median, train_data_mean, train_data_median, pollutants):\n",
    "\n",
    "    models_mean = {}\n",
    "    models_median = {}\n",
    "\n",
    "    for pollutant in pollutants:\n",
    "        # Prepare the target variable for the current pollutant\n",
    "        y_train_mean = train_data_mean[train_data_mean[\"Year\"].isin([2013, 2016, 2019])][pollutant]\n",
    "        y_train_median = train_data_median[train_data_median[\"Year\"].isin([2013, 2016, 2019])][pollutant]\n",
    "\n",
    "        # Apply One-Hot Encoding to the training sets\n",
    "        X_train_mean_encoded = pd.get_dummies(X_train_mean, drop_first=True)\n",
    "        X_train_median_encoded = pd.get_dummies(X_train_median, drop_first=True)\n",
    "\n",
    "        \n",
    "        #Fit linear regression for the mean\n",
    "        linear_regression_mean = LinearRegression()\n",
    "        linear_regression_mean.fit(X_train_mean_encoded, y_train_mean)\n",
    "        models_mean[pollutant] = linear_regression_mean  # Save the trained model\n",
    "\n",
    "        # Save the mean-imputed model to disk\n",
    "        joblib.dump(linear_regression_mean, f'linear_regression_model_mean_{pollutant}.pkl')\n",
    "\n",
    "        #Fit linear regression for the median\n",
    "\n",
    "        #Fit linear regression for the mean\n",
    "        linear_regression_median = LinearRegression()\n",
    "        linear_regression_median.fit(X_train_median_encoded, y_train_median)\n",
    "        models_mean[pollutant] = linear_regression_median  # Save the trained model\n",
    "\n",
    "        # Save the mean-imputed model to disk\n",
    "        joblib.dump(linear_regression_mean, f'linear_regression_model_mean_{pollutant}.pkl')\n",
    "        "
   ],
   "id": "93954ac201c3baf1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Running the function to train the model",
   "id": "efc0b6feae01e679"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T00:24:11.287106Z",
     "start_time": "2024-10-01T00:24:10.304593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_mean = train_data_mean[train_data_mean[\"Year\"].isin([2013, 2016, 2019])].drop(columns=pollutants + [\"Year\"])\n",
    "X_train_median = train_data_median[train_data_median[\"Year\"].isin([2013, 2016, 2019])].drop(columns=pollutants + [\"Year\"])\n",
    "\n",
    "\n",
    "# Train Random Forest models for each pollutant and save them\n",
    "models_mean, models_median = train_linear_regression_for_all_polutants(X_train_mean, X_train_median, train_data_mean, train_data_median, pollutants)"
   ],
   "id": "bb71466d175e105e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Grid ID 2019  LAEI 1km2 ID  Easting  Northing    Borough     Zone  \\\n",
      "285264             1          5910   510500    203500    Non GLA  Non GLA   \n",
      "285265             2          5911   511500    203500    Non GLA  Non GLA   \n",
      "285266             3          5912   512500    203500    Non GLA  Non GLA   \n",
      "285267             4          5915   515500    203500    Non GLA  Non GLA   \n",
      "285268             5          5916   516500    203500    Non GLA  Non GLA   \n",
      "...              ...           ...      ...       ...        ...      ...   \n",
      "699115          3456         10059   531500    179500  Southwark    Inner   \n",
      "699116          3457         10059   531500    179500  Southwark  Central   \n",
      "699117          3458          9714   530500    181500     Camden  Central   \n",
      "699118          3459          9716   532500    181500  Islington  Central   \n",
      "699119          3460          9716   532500    181500       City  Central   \n",
      "\n",
      "       Main Source Category          Sector        Source  bap  ...  hcl  hg  \\\n",
      "285264             Domestic         Biomass  Wood Burning  NaN  ...  NaN NaN   \n",
      "285265             Domestic         Biomass  Wood Burning  NaN  ...  NaN NaN   \n",
      "285266             Domestic         Biomass  Wood Burning  NaN  ...  NaN NaN   \n",
      "285267             Domestic         Biomass  Wood Burning  NaN  ...  NaN NaN   \n",
      "285268             Domestic         Biomass  Wood Burning  NaN  ...  NaN NaN   \n",
      "...                     ...             ...           ...  ...  ...  ...  ..   \n",
      "699115            Transport  Road Transport       TfL Bus  NaN  ...  NaN NaN   \n",
      "699116            Transport  Road Transport       TfL Bus  NaN  ...  NaN NaN   \n",
      "699117            Transport  Road Transport       TfL Bus  NaN  ...  NaN NaN   \n",
      "699118            Transport  Road Transport       TfL Bus  NaN  ...  NaN NaN   \n",
      "699119            Transport  Road Transport       TfL Bus  NaN  ...  NaN NaN   \n",
      "\n",
      "        n2o  nh3  nmvoc  pb  pcb  so2  Emissions Unit  main_source_encoded  \n",
      "285264  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    0  \n",
      "285265  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    0  \n",
      "285266  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    0  \n",
      "285267  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    0  \n",
      "285268  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    0  \n",
      "...     ...  ...    ...  ..  ...  ...             ...                  ...  \n",
      "699115  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    4  \n",
      "699116  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    4  \n",
      "699117  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    4  \n",
      "699118  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    4  \n",
      "699119  NaN  NaN    NaN NaN  NaN  NaN    tonnes/annum                    4  \n",
      "\n",
      "[413856 rows x 26 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(X_train_mean)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Train Random Forest models for each pollutant and save them\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m models_mean, models_median \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_linear_regression_for_all_polutants\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_median\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data_median\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpollutants\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 23\u001B[0m, in \u001B[0;36mtrain_linear_regression_for_all_polutants\u001B[0;34m(X_train_mean, X_train_median, train_data_mean, train_data_median, pollutants)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m#Fit linear regression for the mean\u001B[39;00m\n\u001B[1;32m     22\u001B[0m linear_regression_mean \u001B[38;5;241m=\u001B[39m LinearRegression()\n\u001B[0;32m---> 23\u001B[0m \u001B[43mlinear_regression_mean\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_mean_encoded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_mean\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m models_mean[pollutant] \u001B[38;5;241m=\u001B[39m linear_regression_mean  \u001B[38;5;66;03m# Save the trained model\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Save the mean-imputed model to disk\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/uol-group-d-XPzynESE-py3.12/lib/python3.12/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/uol-group-d-XPzynESE-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_base.py:609\u001B[0m, in \u001B[0;36mLinearRegression.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    605\u001B[0m n_jobs_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs\n\u001B[1;32m    607\u001B[0m accept_sparse \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositive \u001B[38;5;28;01melse\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoo\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 609\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_numeric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmulti_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    618\u001B[0m has_sw \u001B[38;5;241m=\u001B[39m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    619\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_sw:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/uol-group-d-XPzynESE-py3.12/lib/python3.12/site-packages/sklearn/base.py:650\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[1;32m    648\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[1;32m    649\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 650\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m    653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/uol-group-d-XPzynESE-py3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1301\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1296\u001B[0m         estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n\u001B[1;32m   1297\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1298\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1299\u001B[0m     )\n\u001B[0;32m-> 1301\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1302\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1304\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1306\u001B[0m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1308\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_writeable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1309\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1310\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1311\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_nd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_min_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_min_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_min_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1314\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1315\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1316\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1318\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m   1320\u001B[0m check_consistent_length(X, y)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/uol-group-d-XPzynESE-py3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1064\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m   1058\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1059\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1060\u001B[0m         \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[1;32m   1061\u001B[0m     )\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[0;32m-> 1064\u001B[0m     \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1065\u001B[0m \u001B[43m        \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1066\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1067\u001B[0m \u001B[43m        \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1068\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1069\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1071\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copy:\n\u001B[1;32m   1072\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_numpy_namespace(xp):\n\u001B[1;32m   1073\u001B[0m         \u001B[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/uol-group-d-XPzynESE-py3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:123\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m first_pass_isfinite:\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 123\u001B[0m \u001B[43m_assert_all_finite_element_wise\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmsg_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmsg_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/uol-group-d-XPzynESE-py3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:172\u001B[0m, in \u001B[0;36m_assert_all_finite_element_wise\u001B[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[1;32m    158\u001B[0m     msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    160\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    171\u001B[0m     )\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[0;31mValueError\u001B[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T00:24:11.288415Z",
     "start_time": "2024-10-01T00:24:11.288305Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1673d251ad434b9d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
